{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"B_iAjwPm_mYg"},"source":["# Letter recognition (small size)\n","\n","> Indeed, I once even proposed that the toughest challenge facing AI workers is to answer the question: “What are the letters ‘A’ and ‘I’? - [Douglas R. Hofstadter](https://web.stanford.edu/group/SHR/4-2/text/hofstadter.html) (1995)\n","\n","\n","## notMNIST\n","\n","\n","Data source: [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) (you need to download `notMNIST_small.mat` file):\n","\n","![](http://yaroslavvb.com/upload/notMNIST/nmn.png)\n","\n","> some publicly available fonts and extracted glyphs from them to make a dataset similar to MNIST. There are 10 classes, with letters A-J taken from different fonts.\n","\n","> Approaching 0.5% error rate on notMNIST_small would be very impressive. If you run your algorithm on this dataset, please let me know your results.\n","\n","\n","## So, why not MNIST?\n","\n","Many introductions to image classification with deep learning start with MNIST, a standard dataset of handwritten digits. This is unfortunate. Not only does it not produce a “Wow!” effect or show where deep learning shines, but it also can be solved with shallow machine learning techniques. In this case, plain k-Nearest Neighbors produces more than 97% accuracy (or even 99.5% with some data preprocessing!). Moreover, MNIST is not a typical image dataset – and mastering it is unlikely to teach you transferable skills that would be useful for other classification problems\n","\n","> Many good ideas will not work well on MNIST (e.g. batch norm). Inversely many bad ideas may work on MNIST and no[t] transfer to real [computer vision]. - [François Chollet’s tweet](https://twitter.com/fchollet/status/852594987527045120)"]},{"cell_type":"code","metadata":{"id":"jcAAphar__K6"},"source":["!wget http://yaroslavvb.com/upload/notMNIST/notMNIST_small.mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5bJ8z5dnANsf"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import io"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rsvxhuP0_mYt"},"source":["## Data Loading"]},{"cell_type":"code","source":["data = io.loadmat('notMNIST_small.mat')\n","\n","data"],"metadata":{"id":"EGjiGlIJmt5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = data['images']\n","y = data['labels']"],"metadata":{"id":"TeNorDxNm0kJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape, y.shape"],"metadata":{"id":"ync3Gzrim3VI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resolution = 28\n","classes = 10\n","\n","x = np.transpose(x, (2, 0, 1))\n","print(x.shape)\n","x = x.reshape( (-1, resolution, resolution, 1) )"],"metadata":{"id":"NYB_hI-7m58Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample, x, y, channel\n","x.shape, y.shape"],"metadata":{"id":"FPlIBhtrnUbZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 데이터 살펴보기"],"metadata":{"id":"DGxezpUTnv4G"}},{"cell_type":"code","source":["rand_i = np.random.randint(0, x.shape[0])\n","\n","plt.title( f'idx: {rand_i} , y: {\"ABCDEFGHIJ\"[ int(y[rand_i]) ]}' )\n","plt.imshow( x[rand_i, :, :, 0], cmap='Greys' )\n","plt.show()"],"metadata":{"id":"QrHUkjstndza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rows = 5\n","fig, axes = plt.subplots(rows, classes, figsize=(classes,rows))\n","\n","for letter_id in range(classes) :\n","    letters = x[y==letter_id]      # 0부터 9까지 각 숫자에 맞는 array가 letters에 들어간다.\n","    letters_len = len(letters)\n","\n","    for row_i in range(rows) :\n","        axe = axes[row_i, letter_id]\n","        axe.imshow( letters[np.random.randint(letters_len)], cmap='Greys', interpolation='none')\n","        axe.axis('off')"],"metadata":{"id":"7vYrb85CsSKV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing"],"metadata":{"id":"vZaHfXvD9xE-"}},{"cell_type":"markdown","source":["* Data split\n","    - training set : test set = 8 : 2\n","    - training set : validation set = 8 : 2\n","    - 재연을 위한 난수 고정 : 2023"],"metadata":{"id":"3KQ1J6HQe10N"}},{"cell_type":"code","source":["x.shape, y.shape"],"metadata":{"id":"fR_fcaxjfKT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"3_D1fiAh_I7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8xmR99PK_mY2"},"source":["# splitting data into training and test sets\n","train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=2023)\n","train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=2023)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzL95VmTBHxx"},"source":["train_x.shape, train_y.shape, val_x.shape, val_y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Scaling\n","\n","    - min-max scaling"],"metadata":{"id":"L6wdqmy9xglU"}},{"cell_type":"code","source":["max_n, min_n = train_x.max(), train_x.min()"],"metadata":{"id":"L_H5ZJQ2xgiB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x = (train_x - min_n) / (max_n - min_n)\n","val_x = (val_x - min_n) / (max_n - min_n)\n","test_x = (test_x - min_n) / (max_n - min_n)"],"metadata":{"id":"r0HOD2R_xz20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x.max(), train_x.min()"],"metadata":{"id":"9ciZP9h4x8F1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* One-hot encoding"],"metadata":{"id":"u7VEtyVIxgeW"}},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"CxL4r3qhxDdx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_len = len(np.unique(train_y))\n","class_len"],"metadata":{"id":"UQ7VK412yWPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_y = to_categorical(train_y, class_len)\n","val_y = to_categorical(val_y, class_len)\n","test_y = to_categorical(test_y, class_len)"],"metadata":{"id":"RIRcIJxRyDHM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Data shape 재확인"],"metadata":{"id":"X072_BHJzpcF"}},{"cell_type":"code","source":["train_x.shape, train_y.shape"],"metadata":{"id":"h41qEPDYAoxj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsbeDi79gj9-"},"source":["## **Image Data Augmentation**\n","    \n","- ImageDataGenerator\n","- .flow( )"]},{"cell_type":"code","source":["!mkdir output"],"metadata":{"id":"y_S0XT09i0BA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls output"],"metadata":{"id":"y4pNWmKMi2EE"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iVqUpxbWiyn7"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-ysSY2XC6zD"},"source":["# 데이터 제너레이터를 선언함! 제너레이팅 규칙과 함께!\n","trainIDG = ImageDataGenerator(rescale=1./255,         # 사실 이 부분은 전처리 과정에서 했다.\n","                              zca_whitening=True,     # apply ZCA whitening\n","                              rotation_range=30,      # randomly rotate images in the range (degrees, 0 to 180)\n","                              zoom_range = 0.2,       # randomly zoom image\n","                              width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","                              height_shift_range=0.1, # randomly shift images vertically (fraction of total height)\n","                              horizontal_flip=True,   # randomly flip images\n","                              vertical_flip=True)     # randomly flip images\n","\n","# 옵션에 따라 필요할 수도 있고 그렇지 않을 수 있다.\n","trainIDG.fit(train_x)\n","\n","# 학습 할 때마다, '실시간'으로 데이터를 생성하여 학습에 활용하고, 버리고를 반복할 준비!\n","flow_trainIDG = trainIDG.flow(train_x, train_y,\n","                              batch_size=128,\n","                              save_to_dir='output',\n","                              save_prefix='train',\n","                              save_format='png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls output"],"metadata":{"id":"wwiBF4pyiwSz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valIDG = ImageDataGenerator(rescale=1./255)\n","\n","flow_valIDG = valIDG.flow(val_x, val_y,\n","                          batch_size=128,\n","                          save_to_dir='output',\n","                          save_prefix='val',\n","                          save_format='png')"],"metadata":{"id":"5-P2ZfQO5hAm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnZHnb1uBJWj"},"source":["## Modeling : CNN\n","\n","- 조건\n","    1. Sequential API, Functiona API 중 택일.\n","    2. [이 구조를 미니 버전으로 활용해봐도 좋다.](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99DFA5415B38AC752E)\n","    3. DropOut, BatchNormalization 등의 기능도 같이 활용해보자.\n","    4. Early Stopping을 사용할 것."]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow.keras.backend import clear_session\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout\n","from tensorflow.keras.layers import Conv2D, MaxPool2D"],"metadata":{"id":"vIoGQgPl1Fuy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Sequential API"],"metadata":{"id":"w8vprnF4g9nA"}},{"cell_type":"code","source":["# 1. 세션 클리어\n","clear_session()\n","\n","# 2. 모델 선언\n","model1 = Sequential()\n","\n","# 3. 레이어 블록 조립\n","model1.add( Input(shape=(28,28,1)) )\n","\n","model1.add( Conv2D(filters=64,          # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","                   kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","                   strides=(1,1),       # Conv2D 필터의 이동 보폭\n","                   padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","                   activation='relu',   # 빼먹지 않게 주의!\n","                   ) )\n","model1.add( Conv2D(filters=64,          # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","                   kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","                   strides=(1,1),       # Conv2D 필터의 이동 보폭\n","                   padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","                   activation='relu',   # 빼먹지 않게 주의!\n","                   ) )\n","model1.add( MaxPool2D(pool_size=(2,2),  # Maxpool2D 필터의 가로 세로 사이즈\n","                      strides=(2,2)     # Maxpool2D 필터의 이동 보폭 : 기본적으로 pool_size를 따른다.\n","                      ) )\n","model1.add( BatchNormalization() )\n","model1.add( Dropout(0.2) )\n","\n","model1.add( Conv2D(filters=128,         # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","                   kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","                   strides=(1,1),       # Conv2D 필터의 이동 보폭\n","                   padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","                   activation='relu',   # 빼먹지 않게 주의!\n","                   ) )\n","model1.add( Conv2D(filters=128,         # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","                   kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","                   strides=(1,1),       # Conv2D 필터의 이동 보폭\n","                   padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","                   activation='relu',   # 빼먹지 않게 주의!\n","                   ) )\n","model1.add( MaxPool2D(pool_size=(2,2),  # Maxpool2D 필터의 가로 세로 사이즈\n","                      strides=(2,2)     # Maxpool2D 필터의 이동 보폭 : 기본적으로 pool_size를 따른다.\n","                      ) )\n","model1.add( BatchNormalization() )\n","model1.add( Dropout(0.2) )\n","\n","model1.add( Flatten() )\n","model1.add( Dense(256, activation='relu') )\n","model1.add( Dense(10, activation='softmax') )\n","\n","# 4. 컴파일\n","model1.compile(optimizer='adam', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model1.summary()"],"metadata":{"id":"-ljvPeDC2UHu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Functional API"],"metadata":{"id":"ptCExKiAhAgZ"}},{"cell_type":"code","source":["# 1. 세션 클리어\n","clear_session()\n","\n","# 2. 레이어 엮기\n","il = Input(shape=(28,28,1))\n","\n","hl = Conv2D(filters=64,          # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","            kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","            strides=(1,1),       # Conv2D 필터의 이동 보폭\n","            padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","            activation='relu',   # 빼먹지 않게 주의!\n","            )(il)\n","hl = Conv2D(filters=64,          # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","            kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","            strides=(1,1),       # Conv2D 필터의 이동 보폭\n","            padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","            activation='relu',   # 빼먹지 않게 주의!\n","            )(hl)\n","hl = MaxPool2D(pool_size=(2,2),  # Maxpool2D 필터의 가로 세로 사이즈\n","               strides=(2,2)     # Maxpool2D 필터의 이동 보폭 : 기본적으로 pool_size를 따른다.\n","               )(hl)\n","hl = BatchNormalization()(hl)\n","hl = Dropout(0.2)(hl)\n","\n","hl = Conv2D(filters=128,         # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","            kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","            strides=(1,1),       # Conv2D 필터의 이동 보폭\n","            padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","            activation='relu',   # 빼먹지 않게 주의!\n","            )(hl)\n","hl = Conv2D(filters=128,         # Conv2D 필터를 통해 새롭게 제작하려는 feature map의 수\n","            kernel_size=(3,3),   # Conv2D 필터의 가로 세로 사이즈\n","            strides=(1,1),       # Conv2D 필터의 이동 보폭\n","            padding='same',      # 1. 기존의 사이즈를 보존하겠다. | 2. 외곽의 정보를 조금 더 반영하려고!\n","            activation='relu',   # 빼먹지 않게 주의!\n","            )(hl)\n","hl = MaxPool2D(pool_size=(2,2),  # Maxpool2D 필터의 가로 세로 사이즈\n","               strides=(2,2)     # Maxpool2D 필터의 이동 보폭 : 기본적으로 pool_size를 따른다.\n","               )(hl)\n","hl = BatchNormalization()(hl)\n","hl = Dropout(0.2)(hl)\n","\n","hl = Flatten()(hl)\n","hl = Dense(256, activation='relu')(hl)\n","ol = Dense(10, activation='softmax')(hl)\n","\n","# 3. 모델의 시작과 끝 지정\n","model2 = Model(il, ol)\n","\n","# 4. 컴파일\n","model2.compile(optimizer='adam', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","model2.summary()"],"metadata":{"id":"qdNDLY7qg9RO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Early Stopping"],"metadata":{"id":"Hpnt-37bF_F6"}},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping"],"metadata":{"id":"PU9dUzJX2V-g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["es = EarlyStopping(monitor='val_loss',          # 얼리 스토핑을 적용할 관측 대상\n","                   min_delta=0,                 # Threshold. 설정한 값 이상으로 변해야 성능 개선되었다고 간주.\n","                   patience=5,                  # 성능 개선이 발생하지 않았을 때, 몇 epoch를 더 지켜볼 것인가.\n","                   verbose=1,\n","                   restore_best_weights=True)   # 성능이 가장 좋은 epoch의 가중치를 적용함."],"metadata":{"id":"olJWlT5-L0yX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* .fit( )\n","    - Data Augmentation 과정에서 생성한 ImageDataGenerator를 사용해야 한다."],"metadata":{"id":"NKhZAhT6GCYA"}},{"cell_type":"code","source":["model1.fit(flow_trainIDG,               # 위에서 설정한 ImageDataGenerator를 사용해야 한다!\n","           epochs=1, verbose=1,\n","           validation_data=flow_valIDG, # validation set 역시 ImageDataGenerator를 사용!\n","           callbacks=[es]               # 얼리스토핑 적용\n","           )"],"metadata":{"id":"Vau9RsfDGCGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls output"],"metadata":{"id":"m4DJ_TDDj1xM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* .evaluate( )"],"metadata":{"id":"OADy5fyHGEMd"}},{"cell_type":"code","source":["model1.evaluate(test_x, test_y, verbose=1)"],"metadata":{"id":"fpZMsGCtGEbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* .predict( )"],"metadata":{"id":"UaqlpwpzhKgL"}},{"cell_type":"code","source":["y_pred = model1.predict(test_x)"],"metadata":{"id":"ajdE3dv9hKgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원핫 인코딩 한 것을 다시 묶어주는 코드\n","# 평가 지표 및 실제 데이터 확인을 위해 필요\n","\n","y_pred_arg = np.argmax(y_pred, axis=1)\n","test_y_arg = np.argmax(test_y, axis=1)"],"metadata":{"id":"8EDi6-HZGV7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 평가 지표"],"metadata":{"id":"h0i7gtoKQI2M"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, classification_report"],"metadata":{"id":"V0bKlDKAJO6Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy_score(test_y_arg, y_pred_arg)"],"metadata":{"id":"xKaSctmxJSR_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print( classification_report(test_y_arg, y_pred_arg) )"],"metadata":{"id":"cFCAI30pInVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualization"],"metadata":{"id":"dX0PzsnZGdIh"}},{"cell_type":"markdown","source":["* 실제 데이터 확인"],"metadata":{"id":"iAz06dlD7Gno"}},{"cell_type":"code","source":["letters_str = \"ABCDEFGHIJ\"\n","\n","rand_idx = np.random.randint(0, len(y_pred_arg))\n","test_idx = test_y_arg[rand_idx]\n","pred_idx = y_pred_arg[rand_idx]\n","class_prob = np.floor( y_pred[rand_idx]*100 )\n","\n","print(f'idx = {rand_idx}')\n","print(f'해당 인덱스의 이미지는 {letters_str[test_idx]}')\n","print(f'모델의 예측 : {letters_str[pred_idx]}')\n","print(f'모델의 클래스별 확률 : ')\n","print('-------------------')\n","for idx, val in enumerate(letters_str) :\n","    print(val, class_prob[idx])\n","print('=================================================')\n","\n","if test_y_arg[rand_idx] == y_pred_arg[rand_idx] :\n","    print('정답')\n","else :\n","    print('땡')\n","\n","plt.imshow(test_x[rand_idx], cmap='Greys')\n","plt.show()"],"metadata":{"id":"rDzw-qDE7Eoc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 틀린 이미지만 확인해보기"],"metadata":{"id":"v7lHIf-B9Z_L"}},{"cell_type":"code","source":["temp = (test_y_arg == y_pred_arg)\n","false_idx = np.where(temp==False)[0]\n","false_len = len(false_idx)\n","false_len"],"metadata":{"id":"WnLoTiF0LZEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["letters_str = \"ABCDEFGHIJ\"\n","\n","rand_idx = false_idx[np.random.randint(0, false_len)]\n","test_idx = test_y_arg[rand_idx]\n","pred_idx = y_pred_arg[rand_idx]\n","class_prob = np.floor( y_pred[rand_idx]*100 )\n","\n","print(f'idx = {rand_idx}')\n","print(f'해당 인덱스의 이미지는 {letters_str[test_idx]}')\n","print(f'모델의 예측 : {letters_str[pred_idx]}')\n","print(f'모델의 클래스별 확률 : ')\n","print('-------------------')\n","for idx, val in enumerate(letters_str) :\n","    print(val, class_prob[idx])\n","print('=================================================')\n","\n","if test_y_arg[rand_idx] == y_pred_arg[rand_idx] :\n","    print('정답')\n","else :\n","    print('땡')\n","\n","plt.imshow(test_x[rand_idx], cmap='Greys')\n","plt.show()"],"metadata":{"id":"bWcCLIW2_ACh"},"execution_count":null,"outputs":[]}]}